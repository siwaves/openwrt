From e65716166876516fb2f362123160f668689752dc Mon Sep 17 00:00:00 2001
From: Richard Dai <richard@siliconwaves.com>
Date: Wed, 27 Sep 2023 14:09:59 +0800
Subject: [PATCH 3/3] siwaves: add dma mapping

---
 arch/riscv/Kconfig.socs              |   8 +
 arch/riscv/include/asm/device.h      |  15 +
 arch/riscv/include/asm/dma-mapping.h |  67 +++
 arch/riscv/mm/Makefile               |   1 +
 arch/riscv/mm/dma-mapping-siwaves.c  | 833 +++++++++++++++++++++++++++
 arch/riscv/mm/init.c                 |   3 +
 drivers/mmc/host/sdhci_w3k.h         |   2 +
 7 files changed, 929 insertions(+)
 create mode 100644 arch/riscv/include/asm/device.h
 create mode 100644 arch/riscv/include/asm/dma-mapping.h
 create mode 100644 arch/riscv/mm/dma-mapping-siwaves.c

diff --git a/arch/riscv/Kconfig.socs b/arch/riscv/Kconfig.socs
index 3abf55fff..b7563a9f4 100644
--- a/arch/riscv/Kconfig.socs
+++ b/arch/riscv/Kconfig.socs
@@ -55,4 +55,12 @@ config SOC_SILICONWAVES
 	help
 	  This enables support for Siliconwaves SoC platform hardware.
 
+if SOC_SILICONWAVES
+config SOC_SILICONWAVES_DMA_OPS
+	bool "DMA ops mem for siwaves"
+	default n
+	select ARCH_HAS_SETUP_DMA_OPS
+	select DMA_OPS
+endif
+
 endmenu
diff --git a/arch/riscv/include/asm/device.h b/arch/riscv/include/asm/device.h
new file mode 100644
index 000000000..ede818bcb
--- /dev/null
+++ b/arch/riscv/include/asm/device.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Arch specific extensions to struct device
+ */
+#ifndef ASMRISCV_DEVICE_H
+#define ASMRISCV_DEVICE_H
+
+struct dev_archdata {
+    void *dma_struct;
+    void *coherent_struct;
+};
+
+struct pdev_archdata {
+};
+#endif
diff --git a/arch/riscv/include/asm/dma-mapping.h b/arch/riscv/include/asm/dma-mapping.h
new file mode 100644
index 000000000..1f43a7ee3
--- /dev/null
+++ b/arch/riscv/include/asm/dma-mapping.h
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef ASMRISCV_DMA_MAPPING_H
+#define ASMRISCV_DMA_MAPPING_H
+
+#ifdef __KERNEL__
+
+#include <linux/mm_types.h>
+#include <linux/scatterlist.h>
+
+
+extern const struct dma_map_ops riscv_dma_ops;
+
+static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+{
+	if (IS_ENABLED(CONFIG_MMU))
+		return &riscv_dma_ops;
+	return NULL;
+}
+
+/**
+ * riscv_dma_alloc - allocate consistent memory for DMA
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @size: required memory size
+ * @handle: bus-specific DMA address
+ * @attrs: optinal attributes that specific mapping properties
+ *
+ * Allocate some memory for a device for performing DMA.  This function
+ * allocates pages, and will return the CPU-viewed address, and sets @handle
+ * to be the device-viewed address.
+ */
+extern void *riscv_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
+			   gfp_t gfp, unsigned long attrs);
+
+/**
+ * riscv_dma_free - free memory allocated by riscv_dma_alloc
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @size: size of memory originally requested in dma_alloc_coherent
+ * @cpu_addr: CPU-view address returned from dma_alloc_coherent
+ * @handle: device-view address returned from dma_alloc_coherent
+ * @attrs: optinal attributes that specific mapping properties
+ *
+ * Free (and unmap) a DMA buffer previously allocated by
+ * riscv_dma_alloc().
+ *
+ * References to memory and mappings associated with cpu_addr/handle
+ * during and after this call executing are illegal.
+ */
+extern void riscv_dma_free(struct device *dev, size_t size, void *cpu_addr,
+			 dma_addr_t handle, unsigned long attrs);
+
+/*
+ * The scatter list versions of the above methods.
+ */
+extern int riscv_dma_map_sg(struct device *, struct scatterlist *, int,
+		enum dma_data_direction, unsigned long attrs);
+extern void riscv_dma_unmap_sg(struct device *, struct scatterlist *, int,
+		enum dma_data_direction, unsigned long attrs);
+extern void riscv_dma_sync_sg_for_cpu(struct device *, struct scatterlist *, int,
+		enum dma_data_direction);
+extern void riscv_dma_sync_sg_for_device(struct device *, struct scatterlist *, int,
+		enum dma_data_direction);
+extern int riscv_dma_get_sgtable(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
+extern void riscv_dma_init(void);
+#endif /* __KERNEL__ */
+#endif
diff --git a/arch/riscv/mm/Makefile b/arch/riscv/mm/Makefile
index ac7a25298..103522c99 100644
--- a/arch/riscv/mm/Makefile
+++ b/arch/riscv/mm/Makefile
@@ -9,6 +9,7 @@ endif
 KCOV_INSTRUMENT_init.o := n
 
 obj-y += init.o
+obj-$(CONFIG_SOC_SILICONWAVES_DMA_OPS) += dma-mapping-siwaves.o
 obj-y += extable.o
 obj-$(CONFIG_MMU) += fault.o pageattr.o
 obj-y += cacheflush.o
diff --git a/arch/riscv/mm/dma-mapping-siwaves.c b/arch/riscv/mm/dma-mapping-siwaves.c
new file mode 100644
index 000000000..b587e8b2f
--- /dev/null
+++ b/arch/riscv/mm/dma-mapping-siwaves.c
@@ -0,0 +1,833 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ *  linux/arch/riscv/mm/dma-mapping.c
+ *
+ *  Copyright (C) 2000-2004 Russell King
+ *
+ *  DMA uncached mapping support.
+ */
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/genalloc.h>
+#include <linux/gfp.h>
+#include <linux/errno.h>
+#include <linux/list.h>
+#include <linux/init.h>
+#include <linux/device.h>
+#include <linux/dma-direct.h>
+#include <linux/dma-map-ops.h>
+#include <linux/highmem.h>
+#include <linux/memblock.h>
+#include <linux/slab.h>
+#include <linux/iommu.h>
+#include <linux/io.h>
+#include <linux/vmalloc.h>
+#include <linux/sizes.h>
+#include <linux/cma.h>
+#include <linux/memblock.h>
+#include <asm/device.h>
+#include <linux/iommu-helper.h>
+#include <asm/dma-mapping.h>
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#endif
+
+#define RISCV_COHERENT_MEM
+
+#ifdef SIWAVES_DEBUG
+#define debug(fmt, ...) \
+	do { \
+		printk(KERN_INFO pr_fmt(fmt),##__VA_ARGS__); \
+    } while (0)
+#else
+#define debug(fmt, ...)
+#endif
+
+#ifdef RISCV_COHERENT_MEM
+#define RISCV_DMA_COHERENT_SIZE (64UL<<20)
+#define RISCV_DMA_COHERENT_START 0x70000000UL
+#endif
+
+/* default to 64MB */
+#define RISCV_DMA_CACHE_SIZE (64UL<<20)
+#define DMA_DFAULT_START 0x7C000000UL
+#define RISCV_DMA_SLAB_SHIFT 11
+/* default to 2KB*/
+#define RISCV_DMA_SLAB_SIZE (1 << RISCV_DMA_SLAB_SHIFT)
+
+#ifdef RISCV_COHERENT_MEM
+#define RISCV_DMA_SEGSIZE	(128)
+#else
+//MAX size 8MB
+#define RISCV_DMA_SEGSIZE	(1024*8)
+#endif
+
+#define RISCV_DMA_INVALID_PHYS_ADDR (~(phys_addr_t)0)
+
+static void *riscv_dma_va = NULL;
+
+#ifdef RISCV_COHERENT_MEM
+struct dma_coherent_mem {
+	void		*virt_base;
+	dma_addr_t	device_base;
+	unsigned long	pfn_base;
+	int		size;
+	unsigned long	*bitmap;
+	spinlock_t	spinlock;
+	bool		use_dev_dma_pfn_offset;
+};
+#endif
+
+struct riscv_dma_mem_struct {
+	phys_addr_t start;
+	phys_addr_t end;
+	unsigned long nslabs;
+	unsigned long used;
+	unsigned int index;
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *debugfs;
+#endif
+	spinlock_t lock;
+	bool late_alloc;
+	struct riscv_io_slot {
+		phys_addr_t orig_addr;
+		size_t alloc_size;
+		unsigned int list;
+	} *slots;
+};
+
+static unsigned long default_nslabs = 
+	RISCV_DMA_CACHE_SIZE >> RISCV_DMA_SLAB_SHIFT;
+
+static struct riscv_dma_mem_struct riscv_dma_default_mem = {0};
+
+#ifdef RISCV_COHERENT_MEM
+static struct dma_coherent_mem riscv_dma_coherent_mem= {0};
+#endif
+
+static int riscv_dma_find_slots(struct device *dev, phys_addr_t orig_addr,
+			      size_t alloc_size, unsigned int alloc_align_mask);
+static void riscv_dma_release_slots(struct device *dev, phys_addr_t tlb_addr);
+
+static inline 
+struct riscv_dma_mem_struct *dev_get_dma_memory(struct device *dev)
+{
+	return (struct riscv_dma_mem_struct *)dev->archdata.dma_struct;
+}
+
+static inline void *dma_virt(phys_addr_t phy)
+{
+	return riscv_dma_va + (phy - DMA_DFAULT_START);
+}
+
+static inline unsigned long riscv_dma_offset(unsigned long val)
+{
+	return val & (RISCV_DMA_SEGSIZE - 1);
+}
+
+static inline unsigned long nr_slots(u64 val)
+{
+	return DIV_ROUND_UP(val, RISCV_DMA_SLAB_SIZE);
+}
+
+static unsigned int riscv_dma_align_offset(struct device *dev, u64 addr)
+{
+	return addr & dma_get_min_align_mask(dev) & (RISCV_DMA_SLAB_SIZE - 1);
+}
+
+static inline unsigned long get_max_slots(unsigned long boundary_mask)
+{
+	if (boundary_mask == ~0UL)
+		return 1UL << (BITS_PER_LONG - RISCV_DMA_SLAB_SHIFT);
+	return nr_slots(boundary_mask + 1);
+}
+
+static unsigned int wrap_index(struct riscv_dma_mem_struct *mem, unsigned int index)
+{
+	if (index >= mem->nslabs)
+		return 0;
+	return index;
+}
+
+static inline phys_addr_t slot_addr(phys_addr_t start, phys_addr_t idx)
+{
+	return start + (idx << RISCV_DMA_SLAB_SHIFT);
+}
+
+void *riscv_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
+		    gfp_t gfp, unsigned long attrs)
+{
+	struct riscv_dma_mem_struct *mem = dev_get_dma_memory(dev);
+	int index;
+	if (!mem){
+		pr_err("%s %d: alloc dma mem error\n", __func__, __LINE__);
+		return NULL;
+	}
+	index = riscv_dma_find_slots(dev, 0, size, 0);
+	if (index == -1){
+		pr_err("%s %d: alloc dma mem error\n", __func__, __LINE__);
+		return NULL;
+	}
+	*handle = slot_addr(mem->start, index);
+	return dma_virt(*handle);
+}
+
+static inline bool is_dma_buffer(struct device *dev, phys_addr_t paddr)
+{
+	struct riscv_dma_mem_struct *mem = dev_get_dma_memory(dev);
+
+	return mem && paddr >= mem->start && paddr < mem->end;
+}
+
+void riscv_dma_free(struct device *dev, size_t size, void *cpu_addr,
+		  dma_addr_t handle, unsigned long attrs)
+{
+	if (!is_dma_buffer(dev, handle)){
+		panic("%s %d: handle:0x%lx\n", __func__, __LINE__, (unsigned long)handle);
+	}else
+		riscv_dma_release_slots(dev, handle);
+}
+
+static void riscv_dma_bounce(struct device *dev, phys_addr_t tlb_addr, size_t size,
+			   enum dma_data_direction dir)
+{
+	struct riscv_dma_mem_struct *mem = dev_get_dma_memory(dev);
+	int index = (tlb_addr - mem->start) >> RISCV_DMA_SLAB_SHIFT;
+	phys_addr_t orig_addr = mem->slots[index].orig_addr;
+	size_t alloc_size = mem->slots[index].alloc_size;
+	unsigned char *vaddr = dma_virt(tlb_addr);
+	unsigned int tlb_offset, orig_addr_offset;
+
+	if (orig_addr == RISCV_DMA_INVALID_PHYS_ADDR)
+		return;
+
+	tlb_offset = tlb_addr & (RISCV_DMA_SLAB_SIZE - 1);
+	orig_addr_offset = riscv_dma_align_offset(dev, orig_addr);
+	if (tlb_offset < orig_addr_offset) {
+		dev_WARN_ONCE(dev, 1,
+			"Access before mapping start detected. orig offset %u, requested offset %u.\n",
+			orig_addr_offset, tlb_offset);
+		return;
+	}
+
+	tlb_offset -= orig_addr_offset;
+	if (tlb_offset > alloc_size) {
+		dev_WARN_ONCE(dev, 1,
+			"Buffer overflow detected. Allocation size: %zu. Mapping size: %zu+%u.\n",
+			alloc_size, size, tlb_offset);
+		return;
+	}
+
+	orig_addr += tlb_offset;
+	alloc_size -= tlb_offset;
+
+	if (size > alloc_size) {
+		dev_WARN_ONCE(dev, 1,
+			"Buffer overflow detected. Allocation size: %zu. Mapping size: %zu.\n",
+			alloc_size, size);
+		size = alloc_size;
+	}
+	if(dir == DMA_TO_DEVICE) 
+		memcpy(vaddr, phys_to_virt(orig_addr), size);
+	else
+		memcpy(phys_to_virt(orig_addr), vaddr, size);
+}
+
+static void riscv_dma_release_slots(struct device *dev, phys_addr_t tlb_addr)
+{
+	struct riscv_dma_mem_struct *mem = dev_get_dma_memory(dev);
+	unsigned long flags;
+	unsigned int offset = riscv_dma_align_offset(dev, tlb_addr);
+	int index = (tlb_addr - offset - mem->start) >> RISCV_DMA_SLAB_SHIFT;
+	int nslots = nr_slots(mem->slots[index].alloc_size + offset);
+	int count, i;
+
+	spin_lock_irqsave(&mem->lock, flags);
+	if (index + nslots < ALIGN(index + 1, RISCV_DMA_SEGSIZE))
+		count = mem->slots[index + nslots].list;
+	else
+		count = 0;
+
+	for (i = index + nslots - 1; i >= index; i--) {
+		mem->slots[i].list = ++count;
+		mem->slots[i].orig_addr = RISCV_DMA_INVALID_PHYS_ADDR;
+		mem->slots[i].alloc_size = 0;
+	}
+
+	for (i = index - 1;
+	     riscv_dma_offset(i) != RISCV_DMA_SEGSIZE - 1 && mem->slots[i].list;
+	     i--)
+		mem->slots[i].list = ++count;
+	mem->used -= nslots;
+	spin_unlock_irqrestore(&mem->lock, flags);
+	debug("%s %d: mem->used: %lu, nslots: %u", __func__, __LINE__, mem->used,
+		nslots);
+}
+
+static int riscv_dma_find_slots(struct device *dev, phys_addr_t orig_addr,
+			      size_t alloc_size, unsigned int alloc_align_mask)
+{
+	struct riscv_dma_mem_struct *mem = dev_get_dma_memory(dev);
+	unsigned long boundary_mask = dma_get_seg_boundary(dev);
+	dma_addr_t tbl_dma_addr =
+		phys_to_dma_unencrypted(dev, mem->start) & boundary_mask;
+	unsigned long max_slots = get_max_slots(boundary_mask);
+	unsigned int align_mask =
+		dma_get_min_align_mask(dev) & ~(RISCV_DMA_SLAB_SIZE - 1);
+	unsigned int nslots = nr_slots(alloc_size), stride;
+	unsigned int index, wrap, count = 0, i;
+	unsigned int offset = riscv_dma_align_offset(dev, orig_addr);
+	unsigned long flags;
+
+	BUG_ON(!nslots);
+
+	stride = (align_mask >> RISCV_DMA_SLAB_SHIFT) + 1;
+	if (alloc_size >= PAGE_SIZE)
+		stride = max(stride, stride << (PAGE_SHIFT - RISCV_DMA_SLAB_SHIFT));
+	stride = max(stride, (alloc_align_mask >> RISCV_DMA_SLAB_SHIFT) + 1);
+
+	spin_lock_irqsave(&mem->lock, flags);
+	if (unlikely(nslots > mem->nslabs - mem->used)){
+		pr_debug("%s %d: mem->used: %lu\n", __func__, __LINE__, mem->used);
+		goto not_found;
+	}
+
+	index = wrap = wrap_index(mem, ALIGN(mem->index, stride));
+	do {
+		if (orig_addr &&
+		    (slot_addr(tbl_dma_addr, index) & align_mask) !=
+			    (orig_addr & align_mask)) {
+			index = wrap_index(mem, index + 1);
+			continue;
+		}
+
+		if (!iommu_is_span_boundary(index, nslots,
+					    nr_slots(tbl_dma_addr),
+					    max_slots)) {
+			if (mem->slots[index].list >= nslots)
+				goto found;
+		}
+		index = wrap_index(mem, index + stride);
+	} while (index != wrap);
+
+not_found:
+	spin_unlock_irqrestore(&mem->lock, flags);
+	return -1;
+
+found:
+	for (i = index; i < index + nslots; i++) {
+		mem->slots[i].list = 0;
+		mem->slots[i].alloc_size =
+			alloc_size - (offset + ((i - index) << RISCV_DMA_SLAB_SHIFT));
+	}
+	for (i = index - 1;
+	     riscv_dma_offset(i) != RISCV_DMA_SEGSIZE - 1 &&
+	     mem->slots[i].list; i--)
+		mem->slots[i].list = ++count;
+
+	/*
+	 * Update the indices to avoid searching in the next round.
+	 */
+	if (index + nslots < mem->nslabs)
+		mem->index = index + nslots;
+	else
+		mem->index = 0;
+	mem->used += nslots;
+
+	spin_unlock_irqrestore(&mem->lock, flags);
+	debug("%s %d: mem->used: %lu, nslots: %u", __func__, __LINE__, mem->used,
+		nslots);
+	return index;
+}
+
+static void riscvdma_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,
+		size_t size, enum dma_data_direction dir)
+{
+	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)
+		riscv_dma_bounce(dev, tlb_addr, size, DMA_FROM_DEVICE);
+	else
+		BUG_ON(dir != DMA_TO_DEVICE);
+}
+
+void riscvdma_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,
+		size_t size, enum dma_data_direction dir)
+{
+	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)
+		riscv_dma_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);
+	else
+		BUG_ON(dir != DMA_FROM_DEVICE);
+}
+
+void riscv_dma_unmap_single(struct device *dev, phys_addr_t tlb_addr,
+			      size_t mapping_size, enum dma_data_direction dir,
+			      unsigned long attrs)
+{
+	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
+	    (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))
+		riscv_dma_bounce(dev, tlb_addr, mapping_size, DMA_FROM_DEVICE);
+
+	riscv_dma_release_slots(dev, tlb_addr);
+}
+
+
+phys_addr_t riscv_dma_mem_map_single(struct device *dev, phys_addr_t orig_addr,
+		size_t mapping_size, size_t alloc_size,
+		unsigned int alloc_align_mask, enum dma_data_direction dir,
+		unsigned long attrs)
+{
+	struct riscv_dma_mem_struct *mem = dev_get_dma_memory(dev);
+	unsigned int offset = riscv_dma_align_offset(dev, orig_addr);
+	unsigned int i;
+	int index;
+	phys_addr_t tlb_addr;
+
+	if (!mem || !mem->nslabs)
+		panic("Can not allocate dma buffer earlier and can't now provide you with the DMA bounce buffer");
+
+	if (mem_encrypt_active())
+		pr_warn_once("Memory encryption is active and system is using DMA bounce buffers\n");
+
+	if (mapping_size > alloc_size) {
+		dev_warn_once(dev, "Invalid sizes (mapping: %zd bytes, alloc: %zd bytes)",
+			      mapping_size, alloc_size);
+		return (phys_addr_t)DMA_MAPPING_ERROR;
+	}
+
+	index = riscv_dma_find_slots(dev, orig_addr,
+				   alloc_size + offset, alloc_align_mask);
+	if (index == -1) {
+		if (!(attrs & DMA_ATTR_NO_WARN))
+			dev_warn_ratelimited(dev,
+	"dma buffer is full (sz: %zd bytes), total %lu (slots), used %lu (slots)\n",
+				 alloc_size, mem->nslabs, mem->used);
+		return (phys_addr_t)DMA_MAPPING_ERROR;
+	}
+
+	for (i = 0; i < nr_slots(alloc_size + offset); i++)
+		mem->slots[index + i].orig_addr = slot_addr(orig_addr, i);
+	tlb_addr = slot_addr(mem->start, index) + offset;
+
+	riscv_dma_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);
+	return tlb_addr;
+}
+
+static dma_addr_t riscv_mem_map(struct device *dev, phys_addr_t paddr, size_t size,
+		enum dma_data_direction dir, unsigned long attrs)
+{
+	phys_addr_t riscvdma_addr;
+	dma_addr_t dma_addr;
+
+	riscvdma_addr = riscv_dma_mem_map_single(dev, paddr, size, size, 0, dir,
+			attrs);
+	if (riscvdma_addr == (phys_addr_t)DMA_MAPPING_ERROR){
+		pr_err("%s %d: mapping failed\n", __func__, __LINE__);
+		return DMA_MAPPING_ERROR;
+	}
+
+	/* Ensure that the address returned is DMA'ble */
+	dma_addr = phys_to_dma_unencrypted(dev, riscvdma_addr);
+	if (unlikely(!dma_capable(dev, dma_addr, size, true))) {
+		riscv_dma_unmap_single(dev, riscvdma_addr, size, dir,
+			attrs | DMA_ATTR_SKIP_CPU_SYNC);
+		dev_WARN_ONCE(dev, 1,
+			"riscv dma addr %pad+%zu overflow (mask %llx, bus limit %llx).\n",
+			&dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);
+		return DMA_MAPPING_ERROR;
+	}
+
+	return dma_addr;
+}
+
+static dma_addr_t riscv_dma_map_page(struct device *dev, struct page *page,
+	     unsigned long offset, size_t size, enum dma_data_direction dir,
+	     unsigned long attrs)
+{
+	phys_addr_t phys = page_to_phys(page) + offset;
+	dma_addr_t dma_addr = phys_to_dma(dev, phys);
+	dma_addr_t ret = riscv_mem_map(dev, dma_addr, size, dir, attrs);
+	debug("%s %d:mapping phys 0x%lx to PA:[0x%lx-0x%lx], VA:[0x%lx-0x%lx], size: 0x%lx\n", 
+		__func__, __LINE__, (unsigned long)phys,
+		ret, ret+size, (unsigned long)dma_virt(ret), 
+		size+(unsigned long)dma_virt(ret), size);
+	return ret;
+}
+
+static void riscv_dma_unmap_page(struct device *dev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+	phys_addr_t phys = dma_to_phys(dev, handle);
+
+	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
+		if (is_dma_buffer(dev, phys))
+			riscvdma_sync_single_for_cpu(dev, phys, size, dir);
+		else
+			pr_err("%s %d: unmap: [0x%lx-0x%lx]\n", __func__, __LINE__,
+			(unsigned long)phys, (unsigned long)(phys + size));
+	
+	if (is_dma_buffer(dev, phys)){
+		debug("%s %d: unmap: [0x%lx-0x%lx]\n", __func__, __LINE__,
+			(unsigned long)phys, (unsigned long)(phys + size));
+		riscv_dma_unmap_single(dev, phys, size, dir,
+					attrs | DMA_ATTR_SKIP_CPU_SYNC);
+	}else
+		pr_err("%s %d: unmap: [0x%lx-0x%lx]\n", __func__, __LINE__,
+			(unsigned long)phys, (unsigned long)(phys + size));
+}
+
+static void riscv_dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+		int nents, enum dma_data_direction dir, unsigned long attrs)
+{
+	struct scatterlist *sg;
+	int i;
+
+	for_each_sg(sgl, sg, nents, i)
+		riscv_dma_unmap_page(dev, sg->dma_address, sg_dma_len(sg), dir,
+					 attrs);
+}
+
+int riscv_dma_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
+		enum dma_data_direction dir, unsigned long attrs)
+{
+	int i;
+	struct scatterlist *sg;
+
+	for_each_sg(sgl, sg, nents, i) {
+		sg->dma_address = riscv_dma_map_page(dev, sg_page(sg),
+				sg->offset, sg->length, dir, attrs);
+		if (sg->dma_address == DMA_MAPPING_ERROR)
+			goto out_unmap;
+		sg_dma_len(sg) = sg->length;
+	}
+	return nents;
+
+out_unmap:
+	riscv_dma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
+	return -EIO;
+}
+
+void riscv_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction dir, unsigned long attrs)
+{
+	riscv_dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
+}
+
+static void riscv_dma_sync_single_for_cpu(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	riscvdma_sync_single_for_cpu(dev, handle, size, dir);
+}
+
+static void riscv_dma_sync_single_for_device(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	riscvdma_sync_single_for_device(dev, handle, size, dir);
+}
+
+void riscv_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,
+			int nents, enum dma_data_direction dir)
+{
+	struct scatterlist *sg;
+	int i;
+
+	for_each_sg(sgl, sg, nents, i) {
+		phys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));
+		riscvdma_sync_single_for_cpu(dev, paddr, sg->length,
+						    dir);
+	}
+}
+
+void riscv_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,
+			int nents, enum dma_data_direction dir)
+{
+	struct scatterlist *sg;
+	int i;
+
+	for_each_sg(sgl, sg, nents, i) {
+		phys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));
+		riscvdma_sync_single_for_device(dev, paddr, sg->length,
+						       dir);
+	}
+}
+
+static int riscv_dma_supported(struct device *dev, u64 mask)
+{
+	return true;
+}
+
+static void riscv_dma_init_mem(struct riscv_dma_mem_struct *mem, phys_addr_t start,
+				    unsigned long nslabs, bool late_alloc)
+{
+	unsigned long bytes = nslabs << RISCV_DMA_SLAB_SHIFT, i;
+
+	mem->nslabs = nslabs;
+	mem->start = start;
+	mem->end = mem->start + bytes;
+	mem->index = 0;
+	mem->used = 0;
+	mem->late_alloc = late_alloc;
+	spin_lock_init(&mem->lock);
+	for (i = 0; i < mem->nslabs; i++) {
+		mem->slots[i].list = RISCV_DMA_SEGSIZE - riscv_dma_offset(i);
+		mem->slots[i].orig_addr = RISCV_DMA_INVALID_PHYS_ADDR;
+		mem->slots[i].alloc_size = 0;
+	}
+}
+
+static void print_info(void)
+{
+	struct riscv_dma_mem_struct *mem = &riscv_dma_default_mem;
+
+	if (!mem->nslabs) {
+		pr_warn("No low mem\n");
+		return;
+	}
+
+	pr_info("siwaves dma mapped [mem %pa-%pa] (%luMB)\n", &mem->start, &mem->end,
+	       (mem->nslabs << RISCV_DMA_SLAB_SHIFT) >> 20);
+}
+
+int __init __riscv_dma_init(phys_addr_t dma_start, unsigned long nslabs, int verbose)
+{
+	struct riscv_dma_mem_struct *mem = &riscv_dma_default_mem;
+	size_t alloc_size;
+
+	if (WARN_ON_ONCE(mem->nslabs))
+		return -ENOMEM;
+
+	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
+	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
+	if (!mem->slots)
+		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
+		      __func__, alloc_size, PAGE_SIZE);
+	riscv_dma_init_mem(mem, dma_start, nslabs, false);
+	if (verbose)
+		print_info();
+	return 0;
+}
+#ifdef RISCV_COHERENT_MEM
+static void *riscv_dma_alloc_from_global_coherent(struct device *dev, ssize_t size,
+				     dma_addr_t *dma_handle);
+void *riscv_dma_coherent_alloc(struct device *dev, size_t size, dma_addr_t *handle,
+		    gfp_t gfp, unsigned long attrs)
+{
+	return riscv_dma_alloc_from_global_coherent(dev, size, handle);
+}
+static int riscv_dma_release_from_dev_coherent(struct device *dev, size_t size, void *vaddr);
+void riscv_dma_coherent_free(struct device *dev, size_t size, void *cpu_addr,
+		  dma_addr_t handle, unsigned long attrs)
+{
+	riscv_dma_release_from_dev_coherent(dev, size, cpu_addr);
+}
+static int riscv_dma_init_coherent_memory(phys_addr_t phys_addr,
+		dma_addr_t device_addr, size_t size, bool use_dma_pfn_offset);
+#endif
+const struct dma_map_ops riscv_dma_ops = {
+#ifdef RISCV_COHERENT_MEM
+	.alloc					= riscv_dma_coherent_alloc,
+	.free					= riscv_dma_coherent_free,
+#else
+	.alloc					= riscv_dma_alloc,
+	.free					= riscv_dma_free,
+#endif
+	.alloc_pages			= dma_direct_alloc_pages,
+	.free_pages				= dma_direct_free_pages,
+//	.mmap					= riscv_dma_mmap,
+//	.get_sgtable			= riscv_dma_get_sgtable,
+	.map_page				= riscv_dma_map_page,
+	.unmap_page				= riscv_dma_unmap_page,
+	.map_sg					= riscv_dma_map_sg,
+	.unmap_sg				= riscv_dma_unmap_sg,
+	.map_resource			= dma_direct_map_resource,
+	.sync_single_for_cpu	= riscv_dma_sync_single_for_cpu,
+	.sync_single_for_device	= riscv_dma_sync_single_for_device,
+	.sync_sg_for_cpu		= riscv_dma_sync_sg_for_cpu,
+	.sync_sg_for_device		= riscv_dma_sync_sg_for_device,
+	.dma_supported			= riscv_dma_supported,
+	.get_required_mask		= dma_direct_get_required_mask,
+};
+EXPORT_SYMBOL(riscv_dma_ops);
+
+static const struct dma_map_ops *riscv_get_dma_map_ops(bool coherent)
+{
+	return &riscv_dma_ops;
+}
+
+void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+			const struct iommu_ops *iommu, bool coherent)
+{
+	const struct dma_map_ops *dma_ops;
+	dev->archdata.dma_struct = (void*)&riscv_dma_default_mem;
+#ifdef RISCV_COHERENT_MEM
+	dev->archdata.coherent_struct = (void*)&riscv_dma_coherent_mem;
+#endif
+	if (dev->dma_ops)
+		return;
+	dma_ops = riscv_get_dma_map_ops(coherent);
+
+	set_dma_ops(dev, dma_ops);
+}
+
+void __init riscv_dma_init(void)
+{
+	phys_addr_t dma_start = (phys_addr_t)DMA_DFAULT_START;
+	if (__riscv_dma_init(dma_start, default_nslabs, 1))
+		goto fail_free_mem;
+#ifdef RISCV_COHERENT_MEM
+	if(riscv_dma_init_coherent_memory(RISCV_DMA_COHERENT_START, 
+		RISCV_DMA_COHERENT_START, RISCV_DMA_COHERENT_SIZE, true) < 0)
+		pr_err("siwaves init dma coherent failed\n");
+	
+#endif
+	return;
+fail_free_mem:
+	pr_warn("Cannot allocate buffer");
+}
+
+static void riscv_dma_set_mem_base(void *mem_base)
+{
+	riscv_dma_va = mem_base;
+}
+
+static int __init riscv_dma_init_memory(void)
+{
+	void *mem_base = memremap(DMA_DFAULT_START, RISCV_DMA_CACHE_SIZE, MEMREMAP_WC);
+	if (!mem_base)
+		return -EINVAL;
+	debug("siwaves dma virt mem: [0x%lx-0x%lx]\n", 
+		(unsigned long)mem_base, 
+		(unsigned long)mem_base+RISCV_DMA_CACHE_SIZE);
+	riscv_dma_set_mem_base(mem_base);
+#ifdef RISCV_COHERENT_MEM
+	riscv_dma_coherent_mem.virt_base = 
+		memremap(RISCV_DMA_COHERENT_START, RISCV_DMA_COHERENT_SIZE, MEMREMAP_WC);
+	if(!riscv_dma_coherent_mem.virt_base)
+		memunmap(mem_base);
+#endif
+	return 0;
+}
+core_initcall(riscv_dma_init_memory);
+
+#ifdef RISCV_COHERENT_MEM
+
+static inline struct dma_coherent_mem *dev_get_coherent_memory(struct device *dev)
+{
+	return (struct dma_coherent_mem *)dev->archdata.coherent_struct;
+}
+
+static int riscv_dma_init_coherent_memory(phys_addr_t phys_addr,
+		dma_addr_t device_addr, size_t size, bool use_dma_pfn_offset)
+{
+	struct dma_coherent_mem *dma_mem = &riscv_dma_coherent_mem;
+	int pages = size >> PAGE_SHIFT;
+	int bitmap_size = BITS_TO_LONGS(pages) * sizeof(long);
+
+	if (!size)
+		return -1;
+	dma_mem->bitmap = memblock_alloc(bitmap_size, PAGE_SIZE);
+	if (!dma_mem->bitmap)
+		goto out;
+
+	dma_mem->device_base = device_addr;
+	dma_mem->pfn_base = PFN_DOWN(phys_addr);
+	dma_mem->size = pages;
+	dma_mem->use_dev_dma_pfn_offset = use_dma_pfn_offset;
+	spin_lock_init(&dma_mem->spinlock);
+	pr_info("Reserved memory: init DMA memory pool at %pa, size %zd MiB\n",
+		&phys_addr, size / SZ_1M);
+	return 0;
+out:
+	pr_err("Reserved memory: failed to init DMA memory pool at %pa, size %zd MiB\n",
+		&phys_addr, size / SZ_1M);
+	return -1;
+}
+
+static inline dma_addr_t dma_get_device_base(struct device *dev,
+					     struct dma_coherent_mem * mem)
+{
+	if (mem->use_dev_dma_pfn_offset)
+		return phys_to_dma(dev, PFN_PHYS(mem->pfn_base));
+	return mem->device_base;
+}
+
+static void *__riscv_dma_alloc_from_coherent(struct device *dev,
+				       struct dma_coherent_mem *mem,
+				       size_t size, dma_addr_t *dma_handle)
+{
+	int order = get_order(size);
+	unsigned long flags;
+	int pageno;
+	void *ret;
+
+	spin_lock_irqsave(&mem->spinlock, flags);
+
+	if (unlikely(size > ((dma_addr_t)mem->size << PAGE_SHIFT)))
+		goto err;
+
+	pageno = bitmap_find_free_region(mem->bitmap, mem->size, order);
+	if (unlikely(pageno < 0))
+		goto err;
+
+	*dma_handle = dma_get_device_base(dev, mem) +
+			((dma_addr_t)pageno << PAGE_SHIFT);
+	ret = mem->virt_base + ((dma_addr_t)pageno << PAGE_SHIFT);
+	spin_unlock_irqrestore(&mem->spinlock, flags);
+	memset(ret, 0, size);
+	return ret;
+err:
+	spin_unlock_irqrestore(&mem->spinlock, flags);
+	return NULL;
+}
+
+static void *riscv_dma_alloc_from_global_coherent(struct device *dev, ssize_t size,
+				     dma_addr_t *dma_handle)
+{
+	return __riscv_dma_alloc_from_coherent(dev, dev_get_coherent_memory(dev), 
+		size, dma_handle);
+}
+
+static int __dma_release_from_coherent(struct dma_coherent_mem *mem,
+				       int order, void *vaddr)
+{
+	if (mem && vaddr >= mem->virt_base && vaddr <
+		   (mem->virt_base + ((dma_addr_t)mem->size << PAGE_SHIFT))) {
+		int page = (vaddr - mem->virt_base) >> PAGE_SHIFT;
+		unsigned long flags;
+
+		spin_lock_irqsave(&mem->spinlock, flags);
+		bitmap_release_region(mem->bitmap, page, order);
+		spin_unlock_irqrestore(&mem->spinlock, flags);
+		return 1;
+	}
+	return 0;
+}
+
+static int riscv_dma_release_from_dev_coherent(struct device *dev, size_t size, void *vaddr)
+{
+	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);
+	int order = get_order(size);
+	return __dma_release_from_coherent(mem, order, vaddr);
+}
+#endif
+
+#ifdef CONFIG_DEBUG_FS
+static struct dentry *debugfs_dir;
+static void riscv_dma_create_debugfs_files(struct riscv_dma_mem_struct *mem)
+{
+	debugfs_create_ulong("riscv_dma_nslabs", 0400, mem->debugfs, &mem->nslabs);
+	debugfs_create_ulong("riscv_dma_used", 0400, mem->debugfs, &mem->used);
+}
+
+static int __init riscv_dma_create_default_debugfs(void)
+{
+	struct riscv_dma_mem_struct *mem = &riscv_dma_default_mem;
+
+	debugfs_dir = debugfs_create_dir("riscv_dma", NULL);
+	if (mem->nslabs) {
+		mem->debugfs = debugfs_dir;
+		riscv_dma_create_debugfs_files(mem);
+	}
+	return 0;
+}
+late_initcall(riscv_dma_create_default_debugfs);
+
+#endif
diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
index 6c2f38aac..bd22bf507 100644
--- a/arch/riscv/mm/init.c
+++ b/arch/riscv/mm/init.c
@@ -99,6 +99,9 @@ void __init mem_init(void)
 #endif /* CONFIG_FLATMEM */
 
 	high_memory = (void *)(__va(PFN_PHYS(max_low_pfn)));
+#ifdef CONFIG_SOC_SILICONWAVES_DMA_OPS
+	riscv_dma_init();
+#endif
 	memblock_free_all();
 
 	mem_init_print_info(NULL);
diff --git a/drivers/mmc/host/sdhci_w3k.h b/drivers/mmc/host/sdhci_w3k.h
index 4b9dde7bb..d9fcf3e12 100644
--- a/drivers/mmc/host/sdhci_w3k.h
+++ b/drivers/mmc/host/sdhci_w3k.h
@@ -8,7 +8,9 @@
 
 #include <linux/mmc/host.h>
 
+#ifndef CONFIG_SOC_SILICONWAVES_DMA_OPS
 #define USE_SRAM
+#endif
 
 #ifdef USE_SRAM
 
-- 
2.34.1

